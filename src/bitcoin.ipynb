{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "effaca63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAuthor: Tyler Chin\\n\\nReferences (helped massively): https://www.youtube.com/watch?v=q_HS4s1L8UI&t=1768s\\n\\nA script that enables the discord bot to predict stocks!\\n'"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Author: Tyler Chin\n",
    "\n",
    "References (helped massively): https://www.youtube.com/watch?v=q_HS4s1L8UI&t=1768s\n",
    "\n",
    "A script that enables the discord bot to predict stocks!\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "cddc8320",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import yfinance as yf\n",
    "import random\n",
    "\n",
    "from copy import deepcopy as dc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "b979146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Constants that are used throughtout the script\n",
    "'''\n",
    "#data prep\n",
    "peer_into = 14 # 1 <= n <= days the stocks existed\n",
    "training_split = 0.95 # 0 < n < 1\n",
    "\n",
    "# Datasets\n",
    "test_size = 16\n",
    "\n",
    "#trainin\n",
    "epochs = 10 # 0 < n < infinity\n",
    "lr = 0.1\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "bff48742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processor(peer_into: int, df) -> None:\n",
    "    '''\n",
    "    Prepares the data to be read by LSTM\n",
    "\n",
    "    Creates <peer_into> new rows of data which shift each row down by one\n",
    "    '''\n",
    "    for i in range(1, peer_into): \n",
    "        df[f'Close (t-{i})'] = df['Close'].shift(i) # Loops to shift in each items into place\n",
    "\n",
    "    df.dropna(inplace=True) # drop all values with nan because those are not helpful for the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "d40c4813",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyler\\AppData\\Local\\Temp\\ipykernel_25648\\3351515297.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  yesterdays_close = round(data['Close'][-1].item(), 2)\n"
     ]
    }
   ],
   "source": [
    "btc = yf.Ticker('BTC-USD')\n",
    "data = btc.history(period='max')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "yesterdays_close = round(data['Close'][-1].item(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "ca2e80bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['Open', 'Volume','High','Low', 'Dividends', 'Stock Splits'], axis=1)\n",
    "data_processor(peer_into, data)\n",
    "data = data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "e6bb1bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler((-1, 1))\n",
    "data = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "68eb49e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[:, 1:]\n",
    "X = dc(np.flip(X, axis=1))\n",
    "Y = data[:, 0]\n",
    "\n",
    "original = dc(X)\n",
    "\n",
    "split = int(len(X) * training_split)\n",
    "\n",
    "X_train = np.array(np.empty(peer_into - 1))\n",
    "Y_train = np.array([])\n",
    "\n",
    "X_test = np.array(np.empty(peer_into - 1))\n",
    "Y_test = np.array([])\n",
    "for i in range(len(X)): # randomizes the data a bit\n",
    "    seed = random.randint(0, len(X))\n",
    "\n",
    "    if split >= seed:\n",
    "        X_train = np.vstack([X_train, X[i]])\n",
    "        Y_train = np.append(Y_train, Y[i])\n",
    "        continue\n",
    "    \n",
    "    X_test = np.vstack([X_test, X[i]])\n",
    "    Y_test = np.append(Y_test, Y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "8e032c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "X_test = np.expand_dims(X_test, axis=-1)\n",
    "original = np.expand_dims(original, axis=-1)\n",
    "\n",
    "\n",
    "Y_train = Y_train.reshape((-1, 1))\n",
    "Y_test = Y_test.reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "8bb16f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train).float()\n",
    "Y_train = torch.tensor(Y_train).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "Y_test = torch.tensor(Y_test).float()\n",
    "original = torch.tensor(original).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "02f2715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BTCDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X) - 1\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.Y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "6c1c5f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = BTCDataset(X_train, Y_train)\n",
    "testing_dataset = BTCDataset(X_test, Y_test)\n",
    "\n",
    "train_loader = DataLoader(training_dataset, batch_size=test_size, shuffle=True)\n",
    "test_loader = DataLoader(testing_dataset, batch_size=test_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "fa1bb2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, batch in enumerate(train_loader):\n",
    "    x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "ed77b751",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input, hidden, layers):\n",
    "        super().__init__()\n",
    "        self.hidden = hidden\n",
    "        self.layers = layers\n",
    "        self.lstm = nn.LSTM(input, hidden, layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.layers, batch_size, self.hidden).to(device)\n",
    "        c0 = torch.zeros(self.layers, batch_size, self.hidden).to(device)\n",
    "        output, _ = self.lstm(x, (h0, c0))\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "59ee44fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch():\n",
    "    model.train(True)\n",
    "    overall_loss = 0.0\n",
    "\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "        output = model(x_batch)\n",
    "        loss = loss_function(output, y_batch)\n",
    "        overall_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "10972fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_one_epoch():\n",
    "    model.train(False)\n",
    "    overall_loss = 0.0\n",
    "\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(x_batch)\n",
    "            loss = loss_function(output, y_batch)\n",
    "            overall_loss += loss.item()\n",
    "\n",
    "        avg_loss_across_batches = overall_loss / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "b46a3a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(1, 4, 1)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for e in range(epochs):\n",
    "    train_one_epoch()\n",
    "    validate_one_epoch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003d4da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116977.09, 112162.17)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    predicted = model(original.to(device)).to('cpu').numpy()\n",
    "\n",
    "predicted = predicted.flatten()\n",
    "temp = np.zeros((original.shape[0], peer_into))\n",
    "temp[:, 0] = predicted\n",
    "temp = scaler.inverse_transform(temp)\n",
    "\n",
    "predicted = temp[:, 0]\n",
    "todays_close = round(predicted[-1].item(), 2)\n",
    "yesterdays_close, todays_close\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
